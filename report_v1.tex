
\documentclass[10pt,a4paper]{report}
\usepackage[cm]{fullpage}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subfigure}
\usepackage[linesnumbered,boxed]{algorithm2e}
\usepackage{wrapfig}
\usepackage{array}
\usepackage{color}
\usepackage{CJK}
\usepackage{framed}
\newcommand{\argmax}{\mathop{\rm arg~max}\limits}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
\begin{document}

{%%%
%\sffamily 　
\thispagestyle{empty}
\centering
\Large
%~\vspace{\fill}
{\Large Estimation of Conductivities in a 2D Non-Linear Heat Conduction Problem}


\vspace{0.5cm}



\begin{normalsize}
Aravind Sankaran (351286)\\
Krishnaswamy Veluswamy(11111)\\ 
Ting-Jui Hsu(351218)

\end{normalsize}


\vspace{0.5cm}


\begin{normalsize}
\textit{guided by}\\
Markus Towara

\end{normalsize}

\vspace{0.5cm}


\begin{normalsize}
Software and Tools for Computational Engineering\\ 
Department of Computer Science\\ 
RWTH Aachen University\\ 
D-52056 Aachen, Germany\\

\end{normalsize}

\vspace{0.5cm}


\begin{normalsize}
\textit{for}\\
fulfillment of the course “Simulation Science Laboratory”

\end{normalsize}

\vspace{0.5cm}

{\normalsize $28^{th}$ Febuary 2016}

\vspace{\fill}
\pagebreak 


%in the\\[1em]
%\vspace{3.5cm}
%\vspace{\fill}
}

{
\thispagestyle{empty}
%\sffamily
\normalsize
\begin{center}
{\large \textbf{Abstract}}
\end{center}
The unknown distribution of the heat conductivity along the surface of a material is often important for industrial application and scientific research. It’s difficult or even not possible to directly measure the heat conductivity, while measuring temperature is not so difficult. The purpose of this project is to develop a software tool, that calibrates a 2 dimensional surface with constant density and unknown heat conductivities using experimentally obtained real world measurements of temperature distribution. This tool has been developed using C++.\\
\\
Finite difference methods have been used for solving the non-linear heat conductivity equation with Dirichlet boundary conditions and a constant heat source inside the domain. The conductivity grid resolution has been separated from the resolution of temperature distribution. The solver has been parallelized with MPI by decomposing the domain. Explicit $4^{th}$ order Runge-kutta scheme is accurate up to $4^{th}$ order in time. But the stability of this scheme is limited by the CFL conditions. For larger grid sizes, the time step required to maintain stability becomes smaller. Thus, for longer simulation time and larger grid sizes, the time required to solve the heat equation by this method becomes higher. Fully implicit method is unconditionally stable, but requires solving a linear system for each time step. This method is less scalable than explicit schemes.\\
\\
The conductivity distribution is estimated by minimizing the least square estimate of the difference in calculated and observed temperature distribution using Gradient-descent method. Gradient computations with Finite difference and Adjoint mode of Algorithmic differentiation have been compared. Calculating the gradient by Finite difference becomes tedious if the number of parameters to be estimated becomes larger. However, for the given problem, the computation time of each gradient by Adjoint mode AD is independent of the parameter size, but it is bounded by the memory availability. We have used DCO library for computing the gradient by Algorithmic differentiation.\\
\\
For the purpose of testing, and gathering results, the observed temperature has been taken from the simulation and appended with some noise which is standard normally distributed with variance 0.001. The parameter estimation along the surface of a Steel plate has been tested. However, the Steepest descent method takes too many iterations to converge for the given problem as the parameter size increases. This is because the above mentioned cost function becomes less sensitive as the conductivity points to be estimated on a given surface increases.

}

\tableofcontents
\thispagestyle{empty}


\chapter{Introduction}
%\setcounter{page}{1}
To estimate the variation in conductivity along the surface of a homogeneous material*, the heat conduction along the 2D surface have to be simulated. This chapter explains the details of the problem, the solution approaches and the bottlenecks.\\

\begin{small}
\begin{flushleft}
 *  We assume constant density along the surface of the material under consideration
 \end{flushleft} 
\end{small}

\section{Problem Definition}
Heat is an energy that flows from higher temperature to lower temperature. Conduction is a method of heat transfer that occurs by  the movement of electrons from atom to atom .Thus, densely packed materials have higher conduction. The heat conduction in 2D can be modeled using the following partial differential equation$^{[1]}$:
\begin{equation}
\frac{\partial u}{\partial t}=k \bigtriangledown^2(u)
\end{equation}
where,\\
$u$: Temperature\\
$t$: Time\\
$k$: Thermal diffusivity of the material\\
\\
Heat conduction depends on the property of the material, and the “Thermal diffusivity (k)” encodes this dependence in the above heat equation. Furthermore, thermal diffusivity holds the following equation: 

\begin{equation}
k=\frac{c}{\rho C_p}
\end{equation}
where,\\
$c$: Thermal conductivity\\
$\rho$: Density\\
$C_p$: Specific heat capacity\\
\\
Thermal conductivity is the measure of the quantity of thermal energy which flows through a conductor, in a given time, through a given volume and temperature difference$^{[2]}$. Hence it is a property of the material.

If we consider the heat source term, the equation becomes$^{[1]}$,
\begin{equation}
\frac{\partial u}{\partial t}=(\frac{c}{\rho C_p})\bigtriangledown^2(u)+\frac{Q_x}{\rho C_p}
\end{equation}
where,\\
$Q_x$: Temperature of heat source at point x

\subsection{Why should thermal conductivity of a homogeneous material vary?}
Consider a bar of homogeneous material (constant density) which is heated at one end (constant heat source) ,a sink at the other end and the other two sides being insulated. Physics of linear heat conduction tells us that the temperature at every point in the bar should become constant after reaching steady state. But this is under the assumption that the material under consideration has perfect molecular arrangement. But in reality, nothing in this universe is perfect. There are some minor variations in the temperature even after reaching steady state, because of irregularities in the surface of the material. To model this variation, the heat conductivity, c, is made to depend on the spatial dimension. Since the density and Specific heat capacity are still held constant in equation (1.3), we categorize the material to be homogeneous.  Hence, the given problem should not be confused with parameter estimation in a heterogeneous media, where different materials with varying densities are usually fused into a single bar. 

\subsection{The 2D non-linear heat conduction equation for homogeneous materials}

If c depends on the spatial dimensions, the 2D heat equation becomes
\begin{equation}
\frac{\partial u}{\partial t}=(\frac{1}{\rho C_p})\bigtriangledown^2(cu)+\frac{Q_x}{\rho C_p}
\end{equation}

But, 
\begin{equation}
\bigtriangledown^2(cu)=c_x \bigtriangledown^2(u)+\bigtriangledown(c) \bigtriangledown(u)+u_x \bigtriangledown^2(c)
\end{equation}

If $\Delta c$ is small, the variations, $\Delta k\approx 0$, since $\rho C_p \gg k$. Thus, $\Delta c$ has negligible effect on the heat equation.

Hence,
\begin{equation}
\bigtriangledown^2(cu) \approx c_x \bigtriangledown^2(u)
\end{equation}

Therefore, the 2D heat conduction equation for homogeneous materials is 

\begin{equation}
\frac{\partial u}{\partial t}=(\frac{c_x}{\rho C_p})\bigtriangledown^2(u)+\frac{Q_x}{\rho C_p}
\end{equation}

\section{Solution Approach}
The goal of our project is to determine the variations in conductivity along the surface of a homogeneous material.\\
\\
Solution steps:\\
\\
1)Numerically solve the heat equation using finite difference\\
The above heat equation is discretized in an equidistant 2D grid, and has been solved using explicit Runge-kutta scheme and fully implicit scheme. Each method has its own advantages and disadvantages, which have been discussed in detail. The grid is decomposed and the data is distributed among the processors. Each processor works on its local data and the boundary data in the ghost cells are communicated using MPI \\
\\
2) Estimating the Conductivities\\
To estimate the vector of conductivities, gradient descent approach has been used to minimize the following least square estimate.

\begin{equation}
\argmin_{\substack{U}} y= \sum_{\substack{i}}(U_i-O_i)^2
\end{equation}
Where,\\
$U_i$: Final temperature computed by the solver\\
$O_i$: Observed temperature\\
\\
The gradient computations using Finite difference and Adjoint mode algorithmic differentiation has been compared. We use dco for the computation of the gradient using algorithmic differentiation.

\section{Need for Parallelization and Adjoint Mode AD}
Parameter estimation requires the computation of gradient for every iteration of gradient descent.\\
For our least square estimate in equation (1.8),\\
\begin{center}
$f: R^N\mapsto R$\\
\end{center}
Where,\\
$N =n\ast n$\\
$n$= grid size in each dimension \\
\\
The gradient computation by finite difference requires solving the heat equation N times for every gradient. Hence for larger grid sizes, we have larger number of parameters to estimate, which means, the computation time for each gradient increases with increasing parameter size.

If we compute the gradient by adjoint method of Algorithmic differentiation, the heat equation have to be solved M times, where M is the size of the output vector. For our case, M = 1. Hence we get the gradient in a single run of the solver. However, this requires storing the entire graph of computations and reversing the computation flow. Hence this method is bounded by the amount of available memory. By parallelizing the heat equation and decomposing the domain in a distributed memory architecture, we acquire more memory for computing the adjoint, thereby pushing its upper bound. 

The gradient descent method requires solving the heat equation until convergence. For larger grid sizes, the time required for every solution increases. Hence parallelizing the heat equation is crucial for obtaining the parameters as quickly as possible.
%something need to add for the software part.
%The unknown properties and distribution of the heat conductivity is often a important for industrial application and scientific research. It's difficult or even not possible to directly measure the heat conductivity, while measuring temperature is not so difficult. With the measured temperature distribution and some parameter fitting techniques, we can determine the unknown distribution of heat conductivity.
%
%Our project can be divided into two parts, simulation part and estimation part. In simulation part, we use fully implicit method and explicit Runga Kutta method to solve a 2 dimension heat equation. In estimate part, we take the result from the simulation as input and use steepest descent method to fit back heat conductivity.
% 
%Each of these methods has its own limitation. Explicit method is easy to implement, but not stable. Implicit method is guarantee to be stable, but it is usually computational intensive. For estimation, steepest descent method does not always give the best parameter set. We would discuss more details of these limitations later in the report. 
% 
%We mainly use C++ for our report. For Library, we use Message Passing Interface (MPI) for paralleling  and using dco library for algorithmic differentiation.
%
%The purpose of this report is to study how precise and stable can steepest descent method fit back the parameter by showing some numerical experiments and also trying to accelerate iterations and increase the precision of the final result.  

\chapter{Numerical Schemes for Heat Equation}
\section{Discretization of Heat Equation}
%https://en.wikipedia.org/wiki/Heat_equation
%https://en.wikipedia.org/wiki/Thermal_diffusivity

The main model problem would be discussed in our report is a 2D heat equation. We would like to know the final temperature distribution of the plate, when boundary conditions are given temperature on borders and heat source inside the domain. We approximate the derivatives of this problem with finite difference and we define an equidistant $n \times  n$ spatial grid. 

Generalizing the non linear heat comduction equation in (1.7) to 2 dimensions, we get that the temperature evolution with the time would be governed by the heat equation in the form 
\begin{equation}
{\frac{\partial u}{\partial t}}=k_{x,y}\bigtriangledown^2 u,\forall{x,y,t}\in \Omega,
\end{equation}
%{\frac{\partial u}{\partial t}}=k({\frac{\partial^2 u}{\partial x^2}}+{\frac{\partial^2 u}{\partial y^2}}),\forall{x,y,t}\in \Omega,
with $k_{x,y} = {\frac{c_{x,y}}{\rho C_p}}$. Here $c$ is thermal conductivity, $\rho$ is density and $C_p$ is specific heat capacity. 
We can specify heat conductivity c for each point, i.e. heat conductivity is non-uniform. The function u must satisfy the above heat equation and we expect that the temperature at $t=T_{end}$ is determined by the time evolution of the equation.

With finite difference approximation, we can approximate the partial differential equation on grids points. Using Taylor's series approximation first on x-direction spatial grid we can derive
\begin{equation}
u(x_i+h,t^n)=u(x_i,t^n)+hu_x(x_i,t^n)+{\frac{h^2}{2}}u_{xx}(x_i,t^n)+{\frac{h^3}{6}}u_{xxx}(x_i,t^n)+\mathcal{O}(h^4)
\end{equation}
%\[\]
\begin{equation}
u(x_i-h,t^n)=u(x_i,t^n)-hu_x(x_i,t^n)+{\frac{h^2}{2}}u_{xx}(x_i,t^n)-{\frac{h^3}{6}}u_{xxx}(x_i,t^n)+\mathcal{O}(h^4) .
\end{equation}
%\[\]
Adding up two equation and we get
\begin{equation}
u_{xx}(x_i,t^n)={\frac{u(x_i+h,t^n)-2u(x_i,t^n)+u(x_i,t^n)}{h^2}}+\mathcal{O}(h^2)
\end{equation}
%\[\]
and same for y-direction
\begin{equation}
u_{yy}(y_i,t^n)={\frac{u(y_i+h,t^n)-2u(y_i,t^n)+u(y_i,t^n)}{h^2}}+\mathcal{O}(h^2).
\end{equation}
%\[\]
For time difference, we can also write a Taylor's series approximation.
\begin{equation}
u(x_i,t^n+\tau)=u(x_i,t^n)+\tau u_t(x_i,t^n)+\mathcal{O}(\tau^2) 
\end{equation}
%\[\]
and we get\begin{equation}
{\frac{u(x_i,t^n+\tau)-u(xi,t^n)}{\tau}}=u_t(x_i,t^n)+\mathcal{O}(\tau).
\end{equation} %\[\] 

\subsection{Explicit Runge-Kutta Scheme}
%change u into vertor bold font?
%\mathbf{u}

In explicit finite difference scheme, the temperature evolutes explicitly on the temperature at previous time step. The explicit finite difference discretization is simply obtained by the above Tayler's series approximation: 
\begin{equation}
{\frac{u^{n+1}_{i,j}-u^n_{i,j}}{\tau}}=k_{i,j}({\frac{u^n_{i,j+1}-2u^n_{i,j}+u^n_{i,j-1}}{h^2}}+{\frac{u^n_{i+1,j}-2u^n_{i,j}+u^n_{i-1,j}}{h^2}}).
\end{equation}
\[\]
%Consider adding the grid explanation for i j n.
This can be rearranged as following:
\begin{equation}
u^{n+1}_{i,j}=u^{n}_{i,j}+k_{i,j}{\frac{\tau}{h^2}}(u^n_{i,j+1}+u^n_{i+1,j}-4u^n_{i,j}+u^n_{i,j-1}+u^n_{i-1,j}).
\end{equation}
\[\]

We now get a relation between space and time derivatives at each $(x,t)\in \Omega$ with a discrete approximation. As long as we know $u^n_{i\pm 1,j}$,$u^n_{i,j}$, and $u^n_{i,j\pm 1}$, we can compute $u^{n+1}_{i,j}$ for the following time step. The scheme is completely defined when we apply initial conditions and boundary conditions on it.

In order to get higher accuracy, we implement Runge-Kutta method instead of first-order explicit method in our problem. As first-order explicit method, Runge-Kutta method is still a single-step method, with multiple stages per step however. 

To derive Runge-Kutta, once again we start from Taylor series approximation, 
\begin{equation}
u(t+\tau)=u(t)+\tau u'(t)+{\frac{\tau^2}{2}}u''(t)+\mathcal{O}(\tau^3).
\end{equation}
\[\]
Then we set first derivative as $u'(t)=f(t,u(t))$. From this we can get our second derivative by differentiating it, which is
\begin{equation}
u''(t)=f_t(t,u)+f_u(t,u)u'(t)=f_t(t,u)+f_u(t,u)f(t,u).
\end{equation}
 \[\] We substitute these into our Taylor series approximation and rewrite as following,
\begin{align} %without star align will index equation.
u(t+\tau)
=u(t)+\tau f(t,u)+{\frac{\tau^2}{2}}[f_t(t,u)+f_u(t,u)f(t,u)]+\mathcal{O}(\tau^3)\\
=u(t)+{\frac{\tau}{2}} f(t,u)+{\frac{\tau}{2}}[f(t,u)+\tau f_t(t,u)+\tau f_u(t,u)f(t,u)]+\mathcal{O}(\tau^3).  
\end{align}
With multivariate Taylor series approximation, we know $f(t+\tau,u+\tau f(t,u))=f(t,u)+\tau f_t(t,y)+\tau f_u(t,u)f(t,u)+\mathcal{O}(h^2).$ As a result, we get
\begin{equation}
u(t+\tau)=u(t)+{\frac{\tau}{2}}f(t,u)+{\frac{\tau}{2}}f(t+\tau,u+\tau f(t,u))+\mathcal{O}(\tau^3)
\end{equation}
 \[\] and in simple numerical form
\begin{equation}
 u^n+1=u^n+\tau ({\frac{1}{2}}k_1+{\frac{1}{2}}k_2),
 \end{equation} 
\[\]with 
\begin{align}
k_1=f(t_n,u^n),\\
k_2 = f(t_n+\tau, u^n+\tau k_1).
\end{align}
This is so called second-order Runge-Kutta method. However, to gain higher accuracy we use fourthe-order Runge-Kutta method and the method is as follow,
\begin{equation}
u^{n+1}=u^n+h[{\frac{k_1}{6}}+{\frac{k_2}{3}}+{\frac{k_3}{3}}+{\frac{k_4}{6}}]
\end{equation}
\[\]
with

\begin{align*}
k_1=f(t_n,u^n),\\
k_2=f(t_n+{\frac{\tau}{2}},u^n+{\frac{\tau}{2}}k_1),\\
k_2=f(t_n+{\frac{\tau}{2}},u^n+{\frac{\tau}{2}}k_2),\\
k_2=f(t_n+\tau,u^n+\tau k_3).
\end{align*}
The accuracy for the fourth-order Runge-Kutta method is $\mathcal{O}(\tau^5)$. However, this method also requires to evaluate the function $f$ four times per time step. One can easily see that in order to get higher order precision, we need more evaluations per time step. Generally speaking, it's not efficient to apply Runge-Kutta methods higher than fourth order. 

The main benefit of explicit finite difference method is its easiness of implementation and short computing time for each iteration. However, explicit finite difference method is stable only under this condition,
\begin{equation}
{\frac{k_{i,j}\tau}{h^2}}\leq {\frac{1}{4}}.
\end{equation}
\[\] Otherwise, the solution would become unstable and meaningless.\\


\subsection{Fully Implicit Scheme}

To avoid unstable solution, we can use implicit finite difference schemes. We replace forward difference in time with a backward difference and get the implicit discretization of our heat equation as
\begin{equation}
{\frac{u^{n+1}_{i,j}-u^n_{i,j}}{\tau}}=k_{i,j}({\frac{u^{n+1}_{i,j+1}-2u^{n+1}_{i,j}+u^{n+1}_{i,j-1}}{h^2}}+{\frac{u^{n+1}_{i+1,j}-2u^{n+1}_{i,j}+u^{n+1}_{i-1,j}}{h^2}}).
\end{equation}
 \[\]
Once again this can be rearranged into
\begin{equation}
u^{n+1}_{i,j}+k_{i,j}{\frac{\tau}{h^2}}(-u^n_{i,j+1}+-u^n_{i+1,j}+4u^n_{i,j}-u^n_{i,j-1}-u^n_{i-1,j})=u^{n}_{i,j}.
\end{equation}
\[\]
In this scheme, even all $u^n_{i,j}$ are known, we cannot directly compute $u^{n+1}_{i,j} $. We need to solve a linear system at each time step, since all unknown variable $u^{n+1}_{i,j}$ are coupled with each other. We can write our linear system as follow \[\] 
\begin{equation}
AU^{n+1}=U^{n}
\end{equation}

\begin{equation}
U^{n+1}=\begin{bmatrix}
u^{n+1}_{1,1}\\
\vdots\\
u^{n+1}_{1,j_{max}}\\
u^{n+1}_{2,1}\\
\vdots\\
u^{n+1}_{2,j_{max}}\\
\vdots\\
u^{n+1}_{i_{max},j_{max}}\\
\end{bmatrix}
\end{equation}
\[
\]

\begin{equation}
A=\begin{bmatrix}
D_{(i=1)} & E_{(i=1)} & 0  & \cdots  & 0  \\
E_{(i=2)} & D_{(i=2)} & E_{(i=2)} &   & \vdots  \\
  & \ddots & \ddots &\ddots & \\
 \vdots &   & E_{(i=i_{max}-1)} & D_{(i=i_{max}-1)} & E_{(i=i_{max}-1)} \\
 0 & \cdots  &   & E_{(i=i_{max})} & D_{(i=i_{max})}    
\end{bmatrix}
\end{equation}
\[
\]

\begin{equation}
E=\begin{bmatrix}
-{\frac{c_{i,j}\tau}{h^2}} & 0 & & \\
0 & \ddots & \ddots & \\
  & \ddots & \ddots & \\
  &        & 0 & -{\frac{c_{i,j_{max}}\tau}{h^2}}  
\end{bmatrix}
\end{equation}
\[
\]

\begin{equation}
D=\begin{bmatrix}
1+2({\frac{c_{i,j}\tau}{h^2}}+{\frac{c_{i,j}\tau}{h^2}}) & -{\frac{c_{i,j}\tau}{h^2}} & &  \\
-{\frac{c_{i,j+1}\tau}{h^2}} & \ddots & \ddots & \\
& \ddots & \ddots & -{\frac{c_{i,j_{max}-1}\tau}{h^2}}\\
 &  & -{\frac{c_{i,j_{max}}\tau}{h^2}} & 1+2({\frac{c_{i,j_{max}}\tau}{h^2}}+{\frac{c_{i,j_{max}}\tau}{h^2}})
\end{bmatrix}
\end{equation}
\[
\]

\begin{equation}
U^{n}=\begin{bmatrix}
u^{n}_{bottom}\\
\vdots\\
u^{n}_{1,j_{max}}\\
u^{n}_{2,1}\\
\vdots\\
u^{n}_{2,j_{max}}\\
\vdots\\
u^{n}_{bottom}\\
\end{bmatrix}
\end{equation}
\[
\]


As we state in the beginning, the major advantage of implicit finite difference methods is that there is no certain condition for stableness. Therefore, we don't have restriction on the time step size, even if we have high resolution on the spatial part. However, the implicit finite difference method is more computational intensive on each iteration.





\subsection{Domain Decomposition}

In order to improve the efficiency of the code running in the parallelly, we can divide the domain in to different types and assign the division to each of the processor to work on that part of the code or domain. However, the decomposition of domain is of purely personal choice and depends on the user, who programs it. Here we have used 1D domain desomposition, such that the entire domain is split in to rows depending on the number of processors, and the processors act on that part of the domain assigned to them. Below the figure represents the decomposition of the domain.
\pagebreak
\begin{figure}[h]
%\graphicspath{{fig/}}
\begin{center}
\includegraphics[width=0.6\textwidth]{Domaindecomposition} 
\caption{Domain Decomposition} \vskip 10 pt
\label{fig:domain}
\end{center}
\end{figure}


The pseudo code for the domain decomposition is such that
\begin{algorithm}[h]
\caption{Runge-Kutta Scheme with Domain Decomposition}
  \KwIn{}
  \KwOut{}
  
  \While{$t<\text{TIME}$}
  {

	Loop i: 0:my\_end\;	
    \Indp Loop j: 0:my\_end\;
      	\Indp $u_{k+1}$=Runge-Kutta Update($u_{k}$)\;
    \bigskip 
    \Indm \Indm send the top row to the previous rank\\
	receive the last ghost row from the next rank\\
	\bigskip 		
	send the bottom row to the next rank\\
	receive the top ghost row from the previous rank\\   
	\bigskip
    t=t+dt
  }

\end{algorithm}


There at each iteration over the time, we send top row to the previous rank and recieve it in the last ghost row from previous rank. And we also send the bottom row to the next rank and receive it in the top ghost row of the previous rank. This is how the domain is decomposed and the communication of data between the processors occurs.\\
According to the same manner, we can also use it on fully implicit method with Jacobi iteration.\\
The algorithm for the implicit scheme iteration is as follow:

\begin{algorithm}[h]
  \caption{Implicit Scheme Jacobi Iteration}
  \KwIn{}
  \KwOut{}
  
  \While{$t<\text{TIME}$}
  {
    
    \While{$residual<epsilon$}
    {
	Loop i: 1:my\_end\;	
    \Indp Loop j: 1:my\_end\;
      	\Indp $u_{k+1}$=Jacobi-Iteration($u_{k}$)\;
    \Indm \Indm residual=$\parallel x_{k+1}-x_{k} \parallel$  
    }
    t=t+dt
  }
\end{algorithm}
\pagebreak

The above code is compared with the ANSYS Mechanical APDL, and values at the nodes are compared with the RK4 scheme results and the norm of the error is found to be at 1.76 e-7 which is well with in the comparable results. Hence, the RK4 scheme code for 2D heat equation produces similar result to the commercial software, which can validated with. The problem setting was such that the heat/temperature is applied at the top surface of the plate and all other surfaces are insulated. Below is the plot from ANSYS.\\

\begin{figure}
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=3.4in]{Validation.jpg}
\caption{}
\label{fig:side:a}
\end{minipage}%
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=2.2in]{Validation3.png}
\caption{}
\label{fig:side:b}
\end{minipage}
\end{figure}





\section{Internal Heat Source Term}


%http://people.math.gatech.edu/~xchen/teach/pde/heat/Heat-Duhamel.pdf
%https://en.wikipedia.org/wiki/Duhamel%27s_principle

In our problem, boundary conditions are not only the temperature at borders (i.e, the north, east, south and west borders) but also heat sources inside our domain. This basically means we are trying to solve a nonhomogeneous problem instead of homogeneous one only. Our heat equation becomes
\begin{equation}
\begin{cases}
 {\frac{\partial u}{\partial t}}=k_{x,y} ({\frac{\partial^2 u}{\partial x^2}}+{\frac{\partial^2 u}{\partial y^2}})+{\frac{1}{C_p \rho}}q(t) \quad t>0,\\
 u(t=0)=0
\end{cases}
\end{equation}
\[\]  
According to Duhamel's Principle, an auxiliary problem of this is   
\begin{equation}
\begin{cases}
 {\frac{\partial u(t;s)}{\partial t}}=k_{x,y} ({\frac{\partial^2 u(t;s)}{\partial x^2}}+{\frac{\partial^2 u(t;s)}{\partial y^2}}) \quad t>s,\\
 u(s;s)=q(s;s).\\
\end{cases}  
\end{equation}
\[\]
Duhamel's Principle implies that we can move the heat source term into initial conditions and it is possible to derive the solution of a nonhomogeneous heat equation from the solution of a homogeneous heat equation with initial values.

Furthermore, our heat source term is independent of time, so Duhamel's Principle guarantees that we are allowed to absorb the heat source term into initial condition, apply normal numerical method to the problem and will get the correct solution in the end.

\section{Comparison of Numerical Schemes}
??
??




\chapter{Parameter Estimation}
A stable solver which simulates the non-linear heat conduction is required for estimating the heat conductivities, and this was discussed in detail in the last chapter. Now, the conductivities should be optimized to a set of observed temperature distribution. In this chapter, parameter estimation by  gradient descent method will be discussed. 
\section{Steepest Descent Method}

Given a cost function $F(c_1,...,c_n)$ on a search space $c_1,...,c_n \in R$, we update our parameter set,$(c_1,...,c_n)$, until our cost function $F(c_1,...,c_n)$ converges into a minimal. 

%However, it is not guaranteed that the steepest descent method will stop at the real minimal, usually it stops at local minimal then get stuck in there.

In our project, we try to minimize the following square error function function:
%when our hypothesis is $h_c(x)=\mathbf{c}^T \mathbf{x}$,
\begin{equation}
 F(c_1,...,c_n)= \sum\limits_{i=1}^N (U_i-O_i)^2          .
\end{equation} \[\] 
where,\\
$U_i$: Final temperature computed by the solver at $i^{th}$ grid point.\\
$O_i$: Observed temperature at $i^{th}$ grid point.\\
$N$: $n*n$, where n is the grid size in each dimension.\\

%In our project, we try to minimize the following square error function, so the goal would be
%\begin{equation*}
%\argmin_{c_0,...,c_n}F(c_0,...,c_n).
%\end{equation*} 
We use the tabulated values of conductivities as our initial guess,$^{[3]}$ and perform the following update and re-compute the cost function until convergence,
%first, we randomly pick $c_0,...,c_n$ as our start point, then we update each $c$ as following
\begin{equation}
c_j := c_j - \alpha{\frac{\partial F(c_1,...,c_n)}{\partial c_j}},
\end{equation}
where,\\
 $\alpha$ is the learning rate. \\
 
%After updating all parameters, we replace our cost function $F(c_1,...,c_n)$ with the new set of $(c_1,...,c_n)$. The next iteration will start with this new cost function and we iterate it until the cost function converge to its minimum. 

%The algorithm for this is as follow:

\subsection{Finding the Learning Rate}

Line search is performed to find an appropriate learning rate. The learning rate (alpha) is divided by 2, until we find an update (3.2) which reduces the cost function. Even though the monotonous reduction is ensured, the number of free parameter updates that will be required is unclear. If the current alpha is 0.1 and the alpha required for reduction is 0.0001, then at least 500 updates of free parameter would be required. This means, we have to run the simulation 500 times. 

To save so many simulation runs, we can try to solve the following constrained optimization problem, 
\begin{equation}
\min z= y_{k+1}(\alpha_j)-y_k(\alpha_j)   \text{ such that } y_k+1(\alpha_j)<y_k(\alpha_j)
\end{equation}
where,\\
$y_{k+1}$: Least square estimate in the iteration $k+1$ of steepest descent\\
$y_{k}$: Least square estimate in the iteration $k$ of steepest descent\\
$\alpha_j$: $j^{th}$ update of $\alpha$ in $k^{th}$ iteration of steepest descent.

Since the above optimization problem can require the computation of Hessian,  we are not clear at this point, if the complexity of this optimization would be a better trade off. So we resort to the following adaptive update of free parameter in our project as shown in the algorithm,

\begin{algorithm}[t]
\caption{Steepest Descent ($C_0$)}
  \KwIn{Initial Guess $c_0 \in R^n$}
  \KwOut{Updated $c_0$}

  $c=c_{0}$\\
  Compute the gradient $\textbf{g}(c)$\\
  \While{$\parallel \textbf{g} \parallel > \epsilon$}
  {
  line\_search\_count=0 \\
    \While{$F(c_{k})>C(c)$}    
    {
	$c_{k}=c + \alpha \textbf{g}$\\
	Compute $F(c_{k})$\\
	line\_search\_count++ \\
	$\alpha =\textbf{ADAPTIVE\_LINE\_SEARCH(line\_search\_count)}$\\
    }
    $c=c_{k}$\\
    Compute gradient $\textbf{g}(c)$\\
  }
\end{algorithm}
%\vspace{\fill}
%\pagebreak

\begin{algorithm}[h]
\caption{\textbf{ADAPTIVE\_LINE\_SEARCH(line\_search\_count)}}
  \KwIn{line\_search\_count}
  \KwOut{$\alpha$}
  \If{(line\_search\_count<10)}	
  {  
  $\alpha=\frac{\alpha}{2}$
  }
  \ElseIf{(line\_search\_count<10)}	
  {
  $\alpha=\frac{\alpha}{10}$
  }
  \Else	
  {
  $\alpha=\frac{\alpha}{100}$
  }
  Return $\alpha$
\end{algorithm}

However, the implemented adaptive line search may not be optimal for all cases.

\section{Compute the Gradient}

Gradient computation is the biggest bottleneck in gradient based optimization techniques, especially when the number of inputs to the function to be differentiated is large. The gradient of our least square function with respect to its parameters is, 
 
\begin{equation}
g=\frac{\partial F(c)}{c}
\end{equation}       
%For steepest descent, we need to compute the gradient. For this, we have three choice, finite difference, adjoint mode and tagent linear mode. 
%
%Finite difference:
%\begin{equation}
%F(x,\dot{x}):=F'(x)\dot{x}\approx {\frac{F(x+h\dot{x})-F(x)}{h}}
%\end{equation}
%\[\]
%$\text{The computational cost of }F'(x)\, \text{is at} \, O(input) \cdot cost(F)$.
%It is easy and straightforward to use finite difference method. However, when the step size become smaller and smaller, the truncation error will come in, since finite difference is an approximation method. 
%
%Tangent linear mode:
%\begin{equation}
%F(x,\dot{x}):=F'(x)\dot{x}=\dot{y}
%\end{equation}
%\[\]
%$\text{The computational cost of }F'(x)\, \text{is at} \, O(input) \cdot cost(F)$. Although we still need to evaluate function $F$ as many times as finite difference, we can get a more accurate result by using tangent linear mode.
%
%Adjoint mode:
%\begin{equation}
%F(x,\bar{y}):=\bar{y}F'(x)=\bar{x}
%\end{equation}
%\[\]
%$\text{The computational cost of }F'(x)\, \text{is at} \, O(output) \cdot cost(F)$. Different from two methods above, computing gradient with adjoint mode, the computational cost is linear to the output. For our case, it means we can get full gradient in one run. Since accuracy is not the important issue for us, we will mainly use adjoint mode to compute gradient for steepest descent method. 
     
\subsection{Gradient computation using finite difference}

The gradient of the following function can be computed using forward finite difference as follows,\\
$F:R^{N} \mapsto R$,
\begin{algorithm}[h]
\caption{\textbf{FINITE\_DIFFERENCE(c)}}
  \KwIn{conductivity array 'c'}
  \KwOut{gradient g}
  y=F(c')\\
  Loop i: $1:N$\\
  \Indp $\theta_i = \theta_i +h$\\
  $g_{i} = \frac{F(c_{i}-y)}{h}$
  
\end{algorithm}

Hence for a single gradient evaluation, each of the parameter should be perturbed and the least square estimate should be computed. This means the simulation should be run $N$ times for a evaluating one gradient ($N = n*n$, where $n$ is the grid size in each dimension) 


\subsection{Gradient compuataion using Adjoint mode AD}

%\begin{figure}[H]
%\begin{minipage}[t]{0.5\linewidth}
%\centering
%\includegraphics[width=2.2in]{AD_l.png}
%\caption{}
%\label{fig:side:a}
%\end{minipage}%
%\begin{minipage}[t]{0.5\linewidth}
%\centering
%\includegraphics[width=2.2in]{AD_r.png}
%\caption{}
%\label{fig:side:b}
%\end{minipage}
%\end{figure}

\begin{wrapfigure}{r}{0.25\textwidth} %this figure will be at the right
    \centering
    \includegraphics[width=0.25\textwidth]{AD_l.png}
    \caption{}
    \label{AD_l}
\end{wrapfigure}
To compute the gradient by algorithmic differentiation, \\
\\
1) Break down the cost function y = F(c), into sequence of elemental functions. This results in building a directed acyclic graph as shown in Fig.\ref{AD_l} ( This complete graph is not shown, as the structure would complex. But one has to understand that there is a path from each of the input parameter to the output variable)\\
\\
2) Differentiate the elemental functions analytically with respect to its input arguments. This results in a Linearized DAG, with the differentials placed in the edges.\\
\\
3) The derivative of F(c) with respect to ci is obtained by vertex elimination.\\
\\
\begin{equation}
\bigtriangledown F(c) \equiv \frac{\partial y}{\partial c}= \sum_{path \in \text{IDAG}} \; \prod_{(i,j)\in \text{path}} d_{j,i}  
\end{equation}

\begin{wrapfigure}{r}{0.25\textwidth} %this figure will be at the right
    \centering
    \includegraphics[width=0.25\textwidth]{AD_r.png}
    \caption{}
    \label{AD_r}
\end{wrapfigure}

To compute by adjoint mode, we augment this DAG at the output and define\\
\\
y$_{(a1)}$ = (dt/dy)T = 1\\

By chain rule of differentiation,\\
\\
dt/dc = (dt/dy)*(dF/dc)\\
\\
dt/dc = ($y_{a1}$)$^{T} \bigtriangledown $ C = ($\bigtriangledown$ C)$^{T*}$ y$_{a1}$\\ 

Since y$_{a1}$ is a unit scalar,\\
\\
c$_{a1}$  = ($\bigtriangledown$C)$^T$\\
\\
Thus we compute the adjoints of the inputs and recover the gradient in  a single run of simulation.


\subsection{Comparison of gradient computation time by FD and Adjoint AD}
The following graph shows the comparison of computation time of gradient by Finite difference and Adjoint AD. Since we augment the output in the adjoint mode, the computation time of gradient is independent of the input parameter size.\\
\begin{figure}[H]
%\graphicspath{{fig/}}
\begin{center}
\includegraphics[width=0.6\textwidth]{GraDiff.png} 
\caption{} \vskip 10 pt
\label{fig:GradDiff}
\end{center}
\end{figure}

Adjoint AD requires storing the entire graph of simulation. Hence for larger grid size or smaller time steps, this method will break because of memory constraints. Alternatively, one could determine the check points and do call tree reversal$^{[5]}$ to sequentially store parts of the graphs and compute gradient at some extra computational cost. However, this is not done in our project.

\section{Convergence of Steepest Descent}

The considered cost function becomes less sensitive to the parameters as we increase the number of conductivity points to be estimated along a given surface. For example, one cannot expect much change in the least square estimate of temperature difference by changing the conductivity by a small value at just one point in a $10*10$ set. Thus we see this gap between the convergence of Final temperature distribution and convergence of conductivities. Hence, when the solution approaches close to the expected temperature, the gradient values become close to zero, and thereby updating the conductivities in equation (3.2)  by small values at each step of gradient descent.
\begin{figure}[H]
%\graphicspath{{fig/}}
\begin{center}
\includegraphics[width=0.6\textwidth]{Conv.png} 
\caption{} %\vskip 10 pt
\label{fig:Conv}
\end{center}
\end{figure}

%convergence reach a plator and wait long to decrease again.

%\section{Difference between Diffusivity and Thermal Conductivity}
%%https://en.wikipedia.org/wiki/Specific_properties
%%https://www.nde-ed.org/EducationResources/CommunityCollege/Materials/Physical_Chemical/ThermalConductivity.htm
%%In general, steady-state techniques are useful when the temperature of the material does not change with time. => It's depend on temperature, especially on high temperature
%%Thermal conductivity, k, often depends on temperature. Therefore the definitions listed below make sense when the thermal conductivity is temperature independent. Otherwise an representative mean value has to be considered
%
%
%Before we get start, we need to clarify the difference between thermal diffusivity and thermal conductivity. Thermal diffusivity is \[\alpha = {\frac{k}{\rho c_p}}.\] Here $k$ is thermal conductivity, $\rho$ is density and $c_p$ is specific heat capacity. It is also denoted $k$, which is usually misunderstood with thermal conductivity and is not the parameter we want to fit back to our problem. On the other hand, thermal conductivity is a intrinsic property and relate to its ability by conduct heat. Thermal conductivity is often denoted $k$,$\kappa$ and $c$.   
%
%It worth recurring to this, since we mistook these two parameter at the beginning of the project and it lead to the wrong simulation result.

\subsection{Separating the Conductivity Resolution from Temperature Distribution Resolution}
%Graph.
Although we can specify certain thermal conductivity to each points when simulating, at the beginning we are not going to fit parameter on each point. Instead, we divide our domain in to several block and the block size can be specified. The first reason is that usually the original conductivity is not different at each point but block. Second, since in estimation step, we doesn't know the distribution of the initial conductivity, rashly set the resolution to the grid point may cause overfitting and need more computation time. It would be proper to start with big block, and depending on the accuracy requirement, we can reduce the size of the block until we get the expected result.

For example,  the update is made just to each red dotted blocks as shown in the Fig.\ref{fig:CondBlock}. This can speed up the convergence of steepest descent. If gradient is computed using finite difference, this will be of some profit.

\begin{figure}[h]
%\graphicspath{{fig/}}
\begin{center}
\includegraphics[scale=0.45]{CondBlock.png} 
\caption{} \vskip 10 pt
\label{fig:CondBlock}
\end{center}
\end{figure}


Test Case:

Grid Size:\\


\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
 Conductivity Resolution & Iterations to Converge \\ 
 \hline
 $10*10$ & cell6 \\ 
 \hline
 $3*3$   & cell9 \\ 
 \hline
\end{tabular}
\end{center}

\chapter{Conclusion and Suggested Improvement}
%\section{Speed up with MPI}
%\section{Gradient Evaluation with Different Methods}

We presented the software tool for calibrating the surface of a homogeneous material with the heat conductivity variations.

To obtain the correct results, the final temperature at each of the measuring grid point should be well above the initial temperature. Otherwise, the differential at that point will be zero, and the conductivity will not be updated. But for quick simulation, we need to keep the simulation time as less as possible. Therefore, the plate should be heated at high temperature for time as short as possible. Usually for smaller grid sizes, the stability constraints by explicit schemes are pretty relaxed. But when the simulation time is longer, we may have to resort to implicit scheme at the cost of reduced accuracy of final temperature.  

Although the cost function used for estimating the parameters is quite simple because of scalar output, it's sensitivity to the localized conductivities is not sufficient to quickly determine the parameters at all the grid points, thereby resulting in poor convergence. Hence for larger grid sizes, the number of iterations to converge will increase. Alternatively, one could map each blocks of gird separately to the corresponding blocks in observed temperature and try to optimize the multi-objective problem. Otherwise, one could also try to optimize the non-linear least square problem using Gauss-Newton method. But this will require the computation of Jacobian. Since the number of outputs would be same or more than the number of input parameters, Tangent Linear mode AD will be more efficient to compute the Jacobian in this case.

To push the memory constraints of Adjoint AD, the check points of memory break down should be determined and the graphs can be stored in batches  by call tree reversal techniques$^{[5]}$.


\newpage
\appendix
\chapter{ Using DCO for computing the gradient by Adjoint mode AD}

To obtain the gradient with DCO adjoint mode, we have to identify the output of the function to be differentiated and its parameters. The data types should be changed to DCO type for the variables that carries the dependencies from input to output.\\
\\
In our project, we differentiate the following cost function
\definecolor{shadecolor}{rgb}{0.9,0.9,0.9} 
\begin{snugshade}
\begin{flushleft}
template$<$class T$>$\\ 
void f(T *C, T *U\_old, T *U\_new, T \&y)
\end{flushleft}
\end{snugshade}

where C is the array of conductivity values, U\_old and U\_new are the local arrays needed by each processors to store the current and next grid and y is the least square estimate. \\
\\
To differentiate dy/dC with dco, we write the following driver, which returns the gradient 'g' along with the least square estimate 'y'. This runs the simulation in the forward mode and records the graph, and then runs the simulation in reverse mode to recover the gradient values. This means, the communication of ghost cells should also happen in reverse direction.. This is done by the AMPI library(4). We simply replace MPI function with AMPI, and use this call while communicating dco type variables.
\begin{snugshade}
\begin{flushleft}
AMPI\_Sendrecv(\&sendBuf[0], myCol, MPI\_DOUBLE, p\_rank, 0, \&recvBuf[0],myCol,MPI\_DOUBLE,n\_rank,0, m\_cartComm, \&status);
\end{flushleft}
\end{snugshade}
The self contained code for the driver is on the next page.  \\
\\
\begin{figure}[h]
%\graphicspath{{fig/}}
\begin{center}
\includegraphics[width=0.9\textwidth]{DCO.png} 
%\caption{} \vskip 10 pt
\label{fig:DCO}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{snugshade}
%void mpiESTIMATOR::driver\_a1s(double* C,  double *U\_old, double *U\_new, double \&y, double* g) 
%\{ 
%
%    typedef dco::ga1s$<$double$>$ DCO\_M; 
%    typedef typename DCO\_M::type DCO\_T; 
%    typedef typename DCO\_M::tape\_t DCO\_TT; 
%
%/*********************************************************
%* STEP 1: Change the data type to DCO type for the variables required to carry the dependency.
%*
%*  We create variables of dco type and allocate memory and later copy the arrays passed to the driver to these dco type variables.
%*
%*********************************************************
%
%    DCO\_T* ca = new DCO\_T[m\_cSize*m\_cSize]; 
%    DCO\_T ya; 
%    DCO\_T* Ua\_old = new DCO\_T[myRow*myCol]; 
%    DCO\_T* Ua\_new = new DCO\_T[myRow*myCol]; 
%
%    for (size\_t i=0;i$<$myRow*myCol;i++) 
%    \{ 
%        (Ua\_old[i]) = U\_old[i]; 
%        (Ua\_new[i]) = U\_new[i]; 
%    \} 
%
%/***********************************************************
%* STEP 2: We create a tape, that records the graph while forward run of simulation
%*************************************************************
%
%    DCO\_M::global\_tape=DCO\_TT::create(); 
%    DCO\_M::global\_tape-$>$reset(); 
%
%/***********************************************************
%* STEP 3: We register the input parameter to the tape
%*************************************************************
%
%    for (size\_t i=0;i$<$m\_cSize;i++) 
%        for (size\_t j=0;j$<$m\_cSize;j++) 
%        \{ 
%           ca[i*m\_cSize + j]=C[i*m\_cSize + j]; 
%           DCO\_M::global\_tape-$>$register\_variable(ca[i*m\_cSize + j]); 
%        \} 
%
%/***********************************************************
%* STEP 4: Call the function to be differentiated with the DCO type variables to start recording
%*************************************************************
%    f(ca,Ua\_old,Ua\_new,ya); 
%
%/***********************************************************
%* STEP 5: Register the output variable and interpret the adjoint. The adjoint of the output should be set to 1 by rank 0 processor.
%* This runs the simulation in reverse mode. The reverse communication is taken care by AMPI
%************************************************************
%
%    DCO\_M::global\_tape-$>$register\_output\_variable(ya); 
%    y=dco::value(ya); 
%
%    if(my\_rank==0) 
%        dco::derivative(ya)=1; 
%
%
%    DCO\_M::global\_tape-$>$interpret\_adjoint(); 
%
%/*****************************************************************************
%* STEP 6: The local gradient is obtained and the tape is cleared
%******************************************************************************
%
%    for (size\_t i=0;i$<$m\_cSize;i++) 
%        for (size\_t j=0;j$<$m\_cSize;j++) 
%                g[i*m\_cSize + j]=dco::derivative(ca[i*m\_cSize + j]); 
%
%
%    DCO\_TT::remove(DCO\_M::global\_tape); 
%    MPI\_Barrier; 
%
%   for(int i=0; i$<$ m\_cSize*m\_cSize; i++) 
%        MPI\_Allreduce(\&g[i], \&g[i], 1, MPI\_DOUBLE, MPI\_SUM,m\_cartComm); 
%
%
%    delete[] ca; delete[] Ua\_new; delete[] Ua\_old; 
%\}
%
%\end{snugshade}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\newpage

\chapter{ Using the software tool for calibrating the surface of the material}

\begin{wrapfigure}{r}{0.25\textwidth} %this figure will be at the right
    \centering
    \includegraphics[width=0.45\textwidth]{AppenA.png}
\end{wrapfigure}
It is quite simple to use our tool. The parameters of simulation can be set in the python script “main.py”. Hence, one has to modify only this python script. The observed temperature should be  written in a text file named as  “obs\_Temp.txt” under the folder “data” in the following format:\\

$<$ row\_Id $>$ \;     $<$column\_Id$>$  \;   $<$temperature$>$\\

This file should contain the temperature at all grid points including boundary. Hence, for a grid size of 10x10, there should be (10+2)x(10+2) values.

For efficient estimation at each point, the final observed temperature should be well above the initial temperature at every point in the grid. \\

\textit{The followint parameters can be modified:}

%\begin{center}
%\begin{tabular}{ | c {1cm}|c {5cm}|c {2cm}| } 
% \hline
% \textbf{Parameter} & \textbf{description} & \textbf{Unit}\\ 
% \hline
% N & Grid size in each dimension. For eg, if N = 30, 32x32 grid will be initialized including the boundary rows and columns. (Please make sure that this dimension matches with the number of values in “obs\_Tmp.txt”) & -\\ 
% \hline
% $3*3$   & cell9 & clcl\\ 
% \hline
%\end{tabular}
%\end{center}



\begin{tabular}{ | m{10em} | m{7cm}| m{1cm} | } 
\hline
\textbf{Parameter} & \textbf{Description} & \textbf{Unit}\\ 
\hline
N& Grid size in each dimension. For eg, if N = 30, 32x32 grid will be initialized including the boundary rows and columns. (Please make sure that this dimension matches with the number of values in “obs\_Tmp.txt”) & - \\ 
\hline
CN & Conductivity grid size. This can take maximum value of (N+2) for resolution same as the temperature grid. It can take any value less than this, but greater than 0. & - \\ 
\hline
DIMENSION & Dimension of the side of the plate & m \\ 
\hline
TIME & Simulation time & s\\
\hline
RHO & Density of the material of the plate & Kg$m^{-3}$\\
\hline
CP & Specific heat capacity & J$K^{-1}$\\
\hline
NORTH & Temperature at the north boundary & K\\
\hline
SOUTH & Temperature at the south boundary & K\\
\hline
EAST & Temperature at the east boundary & K\\
\hline
WEST & Temperature at the west boundary & K\\
\hline
INITIAL & Initial Temperature & K\\
\hline
C0 & Tabulated value of conductivity of the material mentioned in literature & W/(m. K)\\
\hline
SCHEME & Implicit (i) / Explicit(e) / RK4 (r) & -\\
\hline
GRADIENT & Compute by Finite difference (d) / Adjoint mode AD (a) & -\\
\hline
Q[] & Heat source temperature & K\\
\hline
USE\_SIMULATED\_T & If Observed temperature is not available, temperature from the simulation appended by some noise can be used for estimation & -\\
\hline
\end{tabular}
\newpage

\textbf{Running the python script:}\\

python main.py\\


This will write the required data files and compile the source codes. Two executables will be generated: simulate and estimate. 

In case the observed temperature is not available, one should execute 'simulate' and then execute 'estimate'. Alternatively, executing the python script as \\

python main.py  -\,-test\\

will generate a bash script “run.sh” with the required executable statements.\\

The estimated conductivities will be found under data/estimate\_conductivity.txt and the corresponding final temperature under data/estimate\_Temp.txt

To use the tool just for simulation, use the argument “--simulate” while executing the python script.\\

python main.py -simulate








\end{document}
